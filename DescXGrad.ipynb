{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abrirImagenesEscaladas( carpeta, escala=32 ):\n",
    "    # abre todas las imagenes de la carpeta, y las escala de tal forma que midan (escala x escala)px\n",
    "    # devuelve las imagenes aplanadas -> vectores de tamano escala^2 con valores entre 0 y 1\n",
    "    imagenes = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(carpeta):\n",
    "        for file in filenames:\n",
    "            if file.endswith('DS_Store'):\n",
    "                continue\n",
    "            img = Image.open( os.path.join(carpeta, file) )\n",
    "            img = img.resize((escala, escala))\n",
    "            img.convert('1')\n",
    "            img = np.asarray(img)\n",
    "            if len(img.shape)==3:\n",
    "                img = img[:,:,0].reshape((escala**2 )) / 255\n",
    "            else:\n",
    "                img = img.reshape((escala**2 )) / 255\n",
    "            \n",
    "            imagenes.append( img )\n",
    "\n",
    "    return imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "if(True):\n",
    "    img_train_sin_neumonia = abrirImagenesEscaladas('./chest_xray/train/NORMAL/')\n",
    "    img_train_neumonia = abrirImagenesEscaladas('./chest_xray/train/PNEUMONIA/') # NO FUNCIONA :(\n",
    "    img_test_sin_neumonia = abrirImagenesEscaladas('./chest_xray/test/NORMAL/')\n",
    "    img_test_neumonia = abrirImagenesEscaladas('./chest_xray/test/PNEUMONIA/')\n",
    "\n",
    "    data = (img_train_sin_neumonia, img_train_neumonia, img_test_sin_neumonia, img_test_neumonia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balancear_datos(imagenes_entrenamiento):\n",
    "\n",
    "    img_train_sin_neumonia = imagenes_entrenamiento[0]\n",
    "    img_train_neumonia =imagenes_entrenamiento[1]\n",
    "    img_test_sin_neumonia =imagenes_entrenamiento[2]\n",
    "    img_test_neumonia = imagenes_entrenamiento[3]\n",
    "\n",
    "    # MAX NUMBER OF IMAGES\n",
    "    n_train = min(len(img_train_sin_neumonia), len(img_train_neumonia))\n",
    "    n_test = min(len(img_test_sin_neumonia), len(img_test_neumonia))\n",
    "\n",
    "    # BALANCE\n",
    "    img_train_sin_neumonia = img_train_sin_neumonia[:n_train]\n",
    "    img_train_neumonia = img_train_neumonia[:n_train]\n",
    "    img_test_sin_neumonia = img_test_sin_neumonia[:n_test]\n",
    "    img_test_neumonia = img_test_neumonia[:n_test]\n",
    "\n",
    "    \n",
    "    return (img_train_sin_neumonia, img_train_neumonia, img_test_sin_neumonia, img_test_neumonia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = balancear_datos(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "\n",
    "Derivadas Parciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion f\n",
    "def F(i,w,b):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        i (Vector): imagen reshaped a un vector de tamano 32^2\n",
    "        w (Vector): Pesos de la red\n",
    "        b (Float): Bias de la red\n",
    "\n",
    "    Returns:\n",
    "        probabolidad: 0 < p < 1: Probabilidad de que la imagen sea un 1 (Tiene neumonia)\n",
    "    \"\"\"\n",
    "    tan = np.tanh(np.dot(w,i)+b)\n",
    "    return (tan + 1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivada de L con respecto a W\n",
    "def L_w(i,w,b,d):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        i (Vector): imagen reshaped a un vector de tamano 32^2\n",
    "        w (Vector): Pesos de la red\n",
    "        b (Float): Bias de la red\n",
    "\n",
    "    Returns:\n",
    "        Vector: Gradiente de la probabilidad con respecto a los pesos\n",
    "    \"\"\"\n",
    "    # t0=tanh(b+W⊤⋅i)\n",
    "    #return: (1−t0^2)⋅((1+t0)/2−d)⋅i\n",
    "    \n",
    "    t0 = np.tanh(np.dot(w,i)+b)\n",
    "    return (1-t0**2)*(((1+t0)/2)-d) * i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivada de L con respecto a b\n",
    "def L_b(i,w,b,d):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        i (Vector): imagen reshaped a un vector de tamano 32^2\n",
    "        w (Vector): Pesos de la red\n",
    "        b (Float): Bias de la red\n",
    "\n",
    "    Returns:\n",
    "        Float: Gradiente de la probabilidad con respecto al bias\n",
    "    \"\"\"\n",
    "    #t0=tanh(b+W⊤⋅i)\n",
    "    #(1−t0^2)⋅((1+t0)/2−d)   \n",
    "    t0 = np.tanh(np.dot(w,i)+b)\n",
    "    return (1-t0**2)*(((1+t0)/2)-d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "Implementar el metodo de descenso por gradiente y optimizar los parametros de la funcion f para el conjunto de datos de entrenamiento. Para esto les recomendamos que trabajen con un subconjunto de los datos que tenga una cantidad parecida de imagenes con y sin neumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desenso_gradiente(w,b,gradiente_w, gradiente_b, d, alpha=0.1):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        imagenes_entrenamiento (List): Lista de imagenes de tamano 32^2\n",
    "        w (Vector): Pesos de la red\n",
    "        b (Float): Bias de la red\n",
    "        alpha (Float): Learning rate\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Pesos y bias actualizados\n",
    "\n",
    "    \"\"\"\n",
    "    w = w - alpha * gradiente_w\n",
    "    b = b - alpha * gradiente_b\n",
    "    return w,b\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3\n",
    "\n",
    "Calcular el error cuadratico durante la optimizacion para el conjunto de entrenamiento y para el conjunto de testing. Generar las visualizaciones correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(datos, alpha=0.005, epochs = 5,seed = 42,bool=True):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        datos (tuple): Tupla de dos listas, la primera con las imagenes Normales y la segunda con las imagenes con Neumonia \n",
    "        w (Vector): Pesos de la red\n",
    "        b (Float): Bias de la red\n",
    "        alpha (Float): Learning rate\n",
    "        epochs (Int): Numero de iteraciones\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Pesos y bias actualizados\n",
    "    \"\"\"\n",
    "    # inicioamos con pesos aleatorios\n",
    "    # set numpy seed\n",
    "    \n",
    "\n",
    "    datos_sin_neumonia = datos[0]\n",
    "    datos_con_neumonia = datos[1]\n",
    "\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    w = np.random.randn(datos_sin_neumonia[0].shape[0])\n",
    "    b = np.random.randn(1)\n",
    "    \n",
    "    \n",
    "    print(len(datos_sin_neumonia)+len(datos_con_neumonia),len(datos_sin_neumonia)/len(datos_con_neumonia))\n",
    "    \n",
    "    error = 0\n",
    "    errores = []\n",
    "    for _ in range(epochs):  \n",
    "        error = 0\n",
    "        gradiente_w = np.zeros_like(w)\n",
    "        gradiente_b = 0\n",
    "        \n",
    "        # Entrenamiento con imágenes sin neumonía\n",
    "        label = 0\n",
    "        for i in datos_sin_neumonia: \n",
    "            \n",
    "            gradiente_w += L_w(i,w,b,label)\n",
    "            gradiente_b += L_b(i,w,b,label)\n",
    "            error += (F(i,w,b) - label)**2 # Falta arreglar esto\n",
    "            \n",
    "        # w,b = desenso_gradiente(w,b,gradiente_w,gradiente_b,label,alpha)\n",
    "            \n",
    "        # Entrenamiento con imágenes con neumonía\n",
    "        label = 1\n",
    "        for i in datos_con_neumonia:\n",
    "            gradiente_w += L_w(i,w,b,label)\n",
    "            gradiente_b += L_b(i,w,b,label)\n",
    "            error += (F(i,w,b) - label)**2 # Falta arreglar esto\n",
    "        \n",
    "        # Promediar los gradientes acumulados\n",
    "        gradiente_w /= (len(datos_sin_neumonia) + len(datos_con_neumonia))\n",
    "        gradiente_b /= (len(datos_sin_neumonia) + len(datos_con_neumonia))\n",
    "        \n",
    "        # Actualización de los parámetros\n",
    "        w, b = desenso_gradiente(w, b, gradiente_w, gradiente_b, alpha)\n",
    "        \n",
    "        # Almacenar el error cuadrático para visualización\n",
    "        errores.append(error / (len(datos_sin_neumonia) + len(datos_con_neumonia)))\n",
    "       \n",
    "        \n",
    "        # Decaer la tasa de aprendizaje\n",
    "        alpha *= 0.95\n",
    "        \n",
    "        \n",
    "        # Mostrar el error de la epoch actual\n",
    "        print(f\"\\r{error / (len(datos_sin_neumonia) + len(datos_con_neumonia))}\",end='',)\n",
    "    \n",
    "    if bool:\n",
    "        plt.plot(errores)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Error Cuadrático')\n",
    "        plt.title('Error Cuadrático durante el Entrenamiento')\n",
    "        plt.show()\n",
    "\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(w,b,datos):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        w (Vector): Pesos de la red\n",
    "        b (Float): Bias de la red\n",
    "        datos (tuple): Tupla de dos listas, la primera con las imagenes Normales y la segunda con las imagenes con Neumonia \n",
    "\n",
    "    Returns:\n",
    "        Float: Accuracy\n",
    "    \"\"\"\n",
    "    datos_sin_neumonia = datos[0]\n",
    "    datos_con_neumonia = datos[1]\n",
    "    correctos = 0\n",
    "    for i in datos_sin_neumonia:\n",
    "        if F(i,w,b) < 0.5:\n",
    "            correctos += 1\n",
    "    for i in datos_con_neumonia:\n",
    "        if F(i,w,b) >= 0.5:\n",
    "            correctos += 1\n",
    "    return correctos/(len(datos_sin_neumonia)+len(datos_con_neumonia))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2698 1.0\n",
      "[0.5]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m train_sin, train_con, test_sin, test_con \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m----> 3\u001b[0m w_res, b_res \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_sin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_con\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m test(w_res,b_res,(test_sin,test_con))\n",
      "Cell \u001b[1;32mIn[19], line 42\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(datos, alpha, epochs, seed, bool)\u001b[0m\n\u001b[0;32m     40\u001b[0m     gradiente_w \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m L_w(i,w,b,label)\n\u001b[0;32m     41\u001b[0m     gradiente_b \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m L_b(i,w,b,label)\n\u001b[1;32m---> 42\u001b[0m     error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[43mF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m label)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;66;03m# Falta arreglar esto\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# w,b = desenso_gradiente(w,b,gradiente_w,gradiente_b,label,alpha)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     \n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Entrenamiento con imágenes con neumonía\u001b[39;00m\n\u001b[0;32m     47\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[6], line 13\u001b[0m, in \u001b[0;36mF\u001b[1;34m(i, w, b)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mF\u001b[39m(i,w,b):\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"_summary_\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m        probabolidad: 0 < p < 1: Probabilidad de que la imagen sea un 1 (Tiene neumonia)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     tan \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtanh(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m+\u001b[39mb)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (tan \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_sin, train_con, test_sin, test_con = data\n",
    "\n",
    "w_res, b_res = train(\n",
    "    (train_sin, train_con),\n",
    "    alpha=0.005,\n",
    "    epochs = 500,\n",
    "    seed = 42,\n",
    "    bool=True\n",
    "    )\n",
    "\n",
    "test(w_res,b_res,(test_sin,test_con))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 4\n",
    "Analizar el impacto del parametro α en la convergencia del metodo. Tomar un rango\n",
    "de 5 valores posibles y analizar la convergencia para el conjunto de testing para los distintos valores de α."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_convergencia(errores):\n",
    "    if len(errores) < 2:\n",
    "        return float('inf') \n",
    "    return abs(errores[-1] - errores[-2]) / errores[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_convergencia(datos, alpha,seed,bool=True):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        datos (tuple): Tupla de dos listas, la primera con las imagenes Normales y la segunda con las imagenes con Neumonia \n",
    "        w (Vector): Pesos de la red\n",
    "        b (Float): Bias de la red\n",
    "        alpha (Float): Learning rate\n",
    "        epochs (Int): Numero de iteraciones\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Pesos y bias actualizados\n",
    "    \"\"\"\n",
    "    # inicioamos con pesos aleatorios\n",
    "    # set numpy seed\n",
    "    \n",
    "\n",
    "    datos_sin_neumonia = datos[0]\n",
    "    datos_con_neumonia = datos[1]\n",
    "    \n",
    "\n",
    "    np.random.seed(seed)\n",
    "    w = np.random.randn(datos_sin_neumonia[0].shape[0])\n",
    "    b = np.random.randn(1)\n",
    "    \n",
    "    \n",
    "    print(len(datos_sin_neumonia)+len(datos_con_neumonia),len(datos_sin_neumonia)/len(datos_con_neumonia))\n",
    "    \n",
    "    error = 0\n",
    "    errores = []\n",
    "    contador = 0\n",
    "    while(True):  \n",
    "        error = 0\n",
    "        gradiente_w = np.zeros_like(w)\n",
    "        gradiente_b = 0\n",
    "        \n",
    "        # Entrenamiento con imágenes sin neumonía\n",
    "        label = 0\n",
    "        for i in datos_sin_neumonia: \n",
    "            \n",
    "            gradiente_w += L_w(i,w,b,label)\n",
    "            gradiente_b += L_b(i,w,b,label)\n",
    "            error += (F(i,w,b) - label)**2 # Falta arreglar esto\n",
    "            \n",
    "        # w,b = desenso_gradiente(w,b,gradiente_w,gradiente_b,label,alpha)\n",
    "            \n",
    "        # Entrenamiento con imágenes con neumonía\n",
    "        label = 1\n",
    "        for i in datos_con_neumonia:\n",
    "            gradiente_w += L_w(i,w,b,label)\n",
    "            gradiente_b += L_b(i,w,b,label)\n",
    "            error += (F(i,w,b) - label)**2 # Falta arreglar esto\n",
    "        \n",
    "       \n",
    "        w, b = desenso_gradiente(w, b, gradiente_w, gradiente_b, alpha)\n",
    "        \n",
    "        errores.append(error / (len(datos_sin_neumonia) + len(datos_con_neumonia)))\n",
    "              \n",
    "    \n",
    "        # Decaer la tasa de aprendizaje\n",
    "        alpha *= 0.95\n",
    "        \n",
    "        diferencia = analizar_convergencia(errores)\n",
    "        \n",
    "        # Mostrar el error de la epoch actual\n",
    "        print(f\"\\rError: {errores[-1]}  Diferencia: {diferencia}\",end='',)\n",
    "        \n",
    "        if diferencia < 1e-11:\n",
    "            print(f\"Convergio en el epoch: {contador} con aplha: {alpha}\")\n",
    "            break\n",
    "        \n",
    "        contador += 1\n",
    "       \n",
    "        \n",
    "    if bool:\n",
    "        plt.plot(errores)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Error Cuadrático')\n",
    "        plt.title('Error Cuadrático durante el Entrenamiento')\n",
    "        plt.show()\n",
    "\n",
    "    return w,b,contador,alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "alphas = [0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001]\n",
    "tiempo_ejecucion = []\n",
    "epochs = []\n",
    "alphas_nuevos_valores = []\n",
    "porcentaje_correctitud = []\n",
    "\n",
    "for i in alphas:\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    w_res, b_res, epoch, alpha = train_test_convergencia(\n",
    "        (train_sin, train_con),\n",
    "        alpha=i,\n",
    "        seed = 42,\n",
    "        bool=False\n",
    "        )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    tiempo_ejecucion.append(end_time - start_time)\n",
    "    epochs.append(epoch)\n",
    "    alphas_nuevos_valores.append(alpha)\n",
    "    \n",
    "    porcentaje_correctitud.append(test(w_res,b_res,(test_sin,test_con)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Crear un DataFrame con los datos\n",
    "\n",
    "\n",
    "data = {\n",
    "    'Metric': ['tiempo_ejecucion', 'alphas_nuevos_valores'],\n",
    "    **{alpha: [tiempo_ejecucion[i], alphas_nuevos_valores[i]] for i, alpha in enumerate(alphas)}\n",
    "}\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Nombre del archivo con el valor de convergencia en el título\n",
    "filename = f'results_convergence_{1e-11:.0e}.csv'\n",
    "\n",
    "# Guardar el DataFrame en un archivo CSV\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f'Data saved to {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "alphas_str = [str(alpha) for alpha in alphas]\n",
    "\n",
    "plt.bar(alphas_str, tiempo_ejecucion, color='skyblue')\n",
    "\n",
    "plt.xlabel('Valores de aplha')\n",
    "plt.ylabel('Tiempo de ejecucion (segundos)')\n",
    "plt.title('Tiempos de ejecucion para cada alpha')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "alphas_str = [str(alpha) for alpha in alphas]\n",
    "plt.bar(alphas_str, alphas_nuevos_valores, color='skyblue')\n",
    "\n",
    "plt.xlabel('Valores de aplha')\n",
    "plt.ylabel('Valores finales de alpha')\n",
    "plt.title('Progresion del alpha segun el punto de inicio')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(alphas_str, alphas_nuevos_valores, marker='o', linestyle='-', color='b')  # Gráfico de líneas\n",
    "\n",
    "plt.xlabel('Valores de aplha')\n",
    "plt.ylabel('Valores finales de alpha')\n",
    "plt.title('Progresion del alpha segun el punto de inicio')\n",
    "\n",
    "plt.grid(True) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(alphas_str, epochs, color='skyblue')\n",
    "\n",
    "plt.xlabel('Valores de aplha')\n",
    "plt.ylabel('Iteraciones')\n",
    "plt.title('Cantidad de iteraciones hasta convergencia')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mejor valor de alpha es 0.001 ya que este genera el nuevo valor de alpha mas chico en la menor cantidad de tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos que la convergencia se hago en menos epochs se debe hacer  mas grande el valor de tolerancia (es obvio duh!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 5\n",
    "\n",
    "¿Como impacta el tamaño del escalado de las imagenes en la efectividad del metodo? ¿Y en el tiempo de computo?. Realizar los experimentos y graficos acordes para estudiar estas limitaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "esclados = [32,64,128]\n",
    "tiempo_ejecucion_esclado = []\n",
    "efectividad = []\n",
    "\n",
    "\n",
    "for i in esclados:\n",
    "    img_train_sin_neumonia = abrirImagenesEscaladas('./chest_xray/train/NORMAL/',i)\n",
    "    img_train_neumonia = abrirImagenesEscaladas('./chest_xray/train/PNEUMONIA/',i) # NO FUNCIONA :(\n",
    "    img_test_sin_neumonia = abrirImagenesEscaladas('./chest_xray/test/NORMAL/',i)\n",
    "    img_test_neumonia = abrirImagenesEscaladas('./chest_xray/test/PNEUMONIA/',i)\n",
    "\n",
    "    data = (img_train_sin_neumonia, img_train_neumonia, img_test_sin_neumonia, img_test_neumonia)\n",
    "    \n",
    "    data = balancear_datos(data)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    w_res, b_res = train(\n",
    "        (train_sin, train_con),\n",
    "        alpha=i,\n",
    "        epochs=1500,\n",
    "        seed = 42\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    tiempo_ejecucion_esclado.append(end_time - start_time)\n",
    "    \n",
    "    efectividad.append(test(w_res,b_res,(test_sin,test_con), 0.5))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(esclados, tiempo_ejecucion_esclado, marker='o', linestyle='-', color='b', label='Tiempo de ejecucion')  # Gráfico de líneas\n",
    "\n",
    "plt.xlabel('Esclados')\n",
    "plt.ylabel('Tiempo de ejecucion (segundos)')\n",
    "plt.title('Tiempos de ejecucion para cada esclado')\n",
    "plt.legend()  \n",
    "plt.grid(True) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(esclados, efectividad, marker='o', linestyle='-', color='b', label='Tiempo de ejecucion')  # Gráfico de líneas\n",
    "\n",
    "plt.xlabel('Esclados')\n",
    "plt.ylabel('Tiempo de ejecucion (segundos)')\n",
    "plt.title('Tiempos de ejecucion para cada esclado')\n",
    "plt.legend()  \n",
    "plt.grid(True) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 6 \n",
    "\n",
    "Para el valor de α que tenga mejor valor de convergencia, generar la matriz de confusion y analizar brevemente la efectividad del metodo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "588a077454552c0b608c7a75f739e0012bc0a6317cbb2228f1831255df17ce82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
